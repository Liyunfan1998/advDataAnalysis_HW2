{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# cifa = input_data.read_data_sets(\"../cifar_10\", one_hot=True)\n",
    "data_dir = '/Users/liyunfan/targetDirectory/cifar_10/cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_CIFAR_batch(filename):\n",
    "  \"\"\" load single batch of cifar \"\"\"\n",
    "  with open(filename, 'rb') as f:\n",
    "    datadict = pickle.load(f)\n",
    "    X = datadict['data']\n",
    "    Y = datadict['labels']\n",
    "    X = X.reshape(10000, 3, 32,32).transpose(0,2,3,1).astype(\"float\")\n",
    "    Y = np.array(Y)\n",
    "    return X, Y\n",
    "def load_CIFAR10(ROOT):\n",
    "  \"\"\" load all of cifar \"\"\"\n",
    "  xs = []\n",
    "  ys = []\n",
    "  for b in range(1,6):\n",
    "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "    X, Y = load_CIFAR_batch(f)\n",
    "    xs.append(X)\n",
    "    ys.append(Y)\n",
    "  Xtr = np.concatenate(xs)#使变成行向量\n",
    "  Ytr = np.concatenate(ys)\n",
    "  del X, Y\n",
    "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "  return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, image, label):\n",
    "    batch_image = list()\n",
    "    batch_label = list()\n",
    "    indexs = list()\n",
    "    for i in range(batch_size):\n",
    "        index = random.randint(0, len(image) - 1)\n",
    "        while index in indexs:\n",
    "            index = random.randint(0, len(image) - 1)\n",
    "        d = list(image[index])\n",
    "        batch_image.append(d)\n",
    "        z = label[index]\n",
    "        batch_label.append(z)\n",
    "        indexs.append(index)\n",
    "    return batch_image, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pickle\n",
    "# sys.path.append(\"..\")\n",
    "# sys.path.append(\"./tutorials/\")\n",
    "batch_size = 128\n",
    "XtrAll, YtrAll, XteAll, YteAll = load_CIFAR10(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YtrAll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotY(Y):\n",
    "    Y_new=np.array([[0]*10]*len(Y))\n",
    "    for i in range(len(Y)):\n",
    "        Y_new[i][Y[i]]=1\n",
    "    return Y_new\n",
    "\n",
    "# YtrAll = oneHotY(YtrAll)\n",
    "# YteAll = oneHotY(YteAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape, stddev=5e-2, wl=0):#w1是L1正则中的系数\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    if wl is not None:\n",
    "        # 给weight加一个L2的loss，相当于做了一个L2的正则化处理\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var), wl, name='weight_loss')\n",
    "        # 我们使用tf.add_to_collection把weight loss统一存到一个collection，这个collection名为\"losses\"\n",
    "        # 它会在后面计算神经网络总体loss时被用上\n",
    "        tf.add_to_collection(\"losses\", weight_loss)\n",
    "    return var\n",
    "\"\"\"\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\"\"\"\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "# 卷积函数的四个参数分别是训练图像，卷积核，步长，填充\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# 求最大值池化，长宽缩小一半\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_layer(inputs, w_shape, b_shape, activation_function=None,mul_func = tf.matmul,pool=None,wl=0,stddev=5e-2,norm=False):\n",
    "#     Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "#     biases = tf.Variable(tf.zeros([1, out_size]))\n",
    "    Weights = weight_variable(w_shape,stddev=5e-2,wl=wl)\n",
    "    biases = bias_variable(b_shape)\n",
    "    Wx_plus_b = mul_func(inputs, Weights)+biases\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    if norm:\n",
    "        outputs= tf.nn.lrn(outputs, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "    if pool is not None:\n",
    "        outputs = pool(outputs)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    labels = tf.cast(labels, tf.int64)#真实标签0-9\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=labels, name='cross_entropy_per_example'\n",
    "    )#交叉熵\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy,\n",
    "                                        name='cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)#加到最开始定义的集合这里最后又两部分交叉熵损失函数+L2损失函数\n",
    "\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')#拿到两部分损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "# x = tf.placeholder('float', shape=[batch_size, 32, 32, 3])\n",
    "# y_ = tf.placeholder('float', shape=[batch_size])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 32,32,3])\n",
    "y_ = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_shape1=[3,3,3,64]\n",
    "b_shape1=[64]\n",
    "\n",
    "x_image = tf.reshape(x,[-1, 32,32,3])\n",
    "out1 = add_layer(x_image,w_shape=w_shape1,\n",
    "                 b_shape=b_shape1,mul_func=conv2d,\n",
    "                 activation_function=tf.nn.relu, pool=max_pool_2x2)\n",
    "norm1 = tf.nn.lrn(out1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "w_shape2=[3,3,64,64]\n",
    "b_shape2=[64]\n",
    "\n",
    "out2 = add_layer(out1,w_shape=w_shape2,\n",
    "                 b_shape=b_shape2,mul_func=conv2d,\n",
    "                 activation_function=tf.nn.relu, pool=max_pool_2x2,norm=True)\n",
    "\n",
    "out2_flat=tf.reshape(out2,[batch_size,-1]) # flatten\n",
    "ofdim = out2_flat.get_shape()[1].value\n",
    "print ofdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_shape3=[8* 8 * 64,1024]\n",
    "b_shape3=[1024]\n",
    "\n",
    "# out2_flat = tf.reshape(out2,[-1,ofdim])\n",
    "out3 = add_layer(out2_flat,w_shape=w_shape3,\n",
    "                 b_shape=b_shape3,\n",
    "                 activation_function=tf.nn.relu,stddev=0.04, wl=0.004)\n",
    "\n",
    "# 随机关闭一些神经元防止过拟\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "out3 = tf.nn.dropout(out3, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_shape4 =[1024, 512]\n",
    "b_shape4 = [512]\n",
    "\n",
    "out4 = add_layer(out3,w_shape4,b_shape4,activation_function=tf.nn.relu, stddev=0.04, wl=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100), Dimension(10)])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从1024个神经元映射到10个神经元\n",
    "w_shape5 =[512, 10]\n",
    "b_shape5 = [10]\n",
    "\n",
    "pred = add_layer(out4,w_shape5,b_shape5,tf.nn.softmax, stddev=1/512.0)\n",
    "\n",
    "pred.shape\n",
    "# labels = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "# b=tf.cast(labels, tf.int64)\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.initialize_all_variables())\n",
    "# print(sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100), Dimension(10)])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_=tf.cast(y_,tf.int32)\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=out4, labels=y_)\n",
    "cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "tf.add_to_collection('losses', cross_entropy_mean)\n",
    "loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "tf.summary.scalar('losses',loss)\n",
    "\n",
    "# train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "Xte = tf.placeholder(tf.float32, [None, 32,32,3])\n",
    "Yte = tf.placeholder(tf.int32, [None])\n",
    "# top_k_op = tf.nn.in_top_k(Xte, Yte, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算交叉熵损失\n",
    "# cross_entropy = -tf.reduce_sum(y_*tf.log(pred))\n",
    "# # 创建优化器\n",
    "# train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "# 计算准确率， tf.argmax函数 在 label 中找出数值最大的那个元素的下标\n",
    "# correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(y_,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "top_k_op = tf.nn.in_top_k(out4, y_, 1)\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"summary_op =  tf.summary.merge_all()\\nsess.run(tf.global_variables_initializer())\\nfor step in range(max_steps):\\n    start_time = time.time()\\n    image_batch, label_batch = get_batch(batch_size=batch_size,image=XtrAll,label=YtrAll)\\n    image_batch= np.array(image_batch)\\n    label_batch=1.0*np.array(label_batch)\\n    print image_batch.dtype, label_batch.shape\\n    \\n#     image_batch, label_batch = sess.run([images_train, labels_train])#\\xe7\\x9c\\x9f\\xe6\\xad\\xa3\\xe6\\x89\\xa7\\xe8\\xa1\\x8ctensor\\xe9\\x80\\xbb\\xe8\\xbe\\x91 \\xe8\\xbf\\x94\\xe5\\x9b\\x9e\\xe7\\x9a\\x84\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe6\\x89\\xb9\\xe6\\xac\\xa1\\xe7\\x9a\\x84\\xe6\\x95\\xb0\\xe6\\x8d\\xae\\n    _, loss_value,summary_str = sess.run([train_op, loss,summary_op],feed_dict={x: image_batch, y_: label_batch})\\n    duration = time.time() - start_time\\n    if step % 10 == 0:\\n        examples_per_sec = batch_size / duration\\n        sec_per_batch = float(duration)\\n\\n        format_str = 'step %d, loss = %.2f (%.1f examples/sec; %.3f sec/batch)'\\n        print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "# tf.train.start_queue_runners()\n",
    "\"\"\"summary_op =  tf.summary.merge_all()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(max_steps):\n",
    "    start_time = time.time()\n",
    "    image_batch, label_batch = get_batch(batch_size=batch_size,image=XtrAll,label=YtrAll)\n",
    "    image_batch= np.array(image_batch)\n",
    "    label_batch=1.0*np.array(label_batch)\n",
    "    print image_batch.dtype, label_batch.shape\n",
    "    \n",
    "#     image_batch, label_batch = sess.run([images_train, labels_train])#真正执行tensor逻辑 返回的是一批次的数据\n",
    "    _, loss_value,summary_str = sess.run([train_op, loss,summary_op],feed_dict={x: image_batch, y_: label_batch})\n",
    "    duration = time.time() - start_time\n",
    "    if step % 10 == 0:\n",
    "        examples_per_sec = batch_size / duration\n",
    "        sec_per_batch = float(duration)\n",
    "\n",
    "        format_str = 'step %d, loss = %.2f (%.1f examples/sec; %.3f sec/batch)'\n",
    "        print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def next_batch(datasetX,datasetY,batch_size,nowAt):\n",
    "    return datasetX[nowAt:nowAt+batch_size],datasetY[nowAt:nowAt+batch_size],nowAt+batch_size\"\"\"pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for i in range(20000):\n",
    "    nowAt=0\n",
    "#     print nowAt\n",
    "    Xtr,Ytr,a = next_batch(XtrAll,YtrAll,batch_size,nowAt)\n",
    "    Xte,Yte,nowAt = next_batch(XteAll,YteAll,batch_size,nowAt)\n",
    "    if (i%100 == 0):\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:Xtr, y_: Ytr, keep_prob: 1.0})\n",
    "        print (\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "      # 运行训练模型\n",
    "    train_step.run(feed_dict={x: Xtr, y_: Ytr, keep_prob: 0.5})\n",
    "print (\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "x: Xte, y_: Yte, keep_prob: 1.0}))\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess=tf.InteractiveSession()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#保存模型\n",
    "saver = tf.train.Saver()\n",
    "saveFile='./CNNmodles/cifar10/CNN_cifar10.ckpt'\n",
    "\n",
    "# 使用tensorboard ，保存至LOG文件夹\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('./LOG2' , sess.graph)\n",
    "import random\n",
    "test_a=list()\n",
    "epoch_num=41\n",
    "n_batch = 50000 // batch_size\n",
    "#epoch：迭代次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [100,32,32,3]\n\t [[node Placeholder (defined at <ipython-input-10-3a49e7393409>:5) ]]\n\nCaused by op u'Placeholder', defined at:\n  File \"/usr/local/Cellar/python@2/2.7.15_3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/local/Cellar/python@2/2.7.15_3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tornado/ioloop.py\", line 1017, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 542, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 456, in _handle_events\n    self._handle_recv()\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 486, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 438, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-3a49e7393409>\", line 5, in <module>\n    x = tf.placeholder(tf.float32, [batch_size, 32,32,3])\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 2077, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5791, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [100,32,32,3]\n\t [[node Placeholder (defined at <ipython-input-10-3a49e7393409>:5) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-0816b621044f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbatch_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mbatch_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx_image\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch:\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\" batch:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cross_entropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [100,32,32,3]\n\t [[node Placeholder (defined at <ipython-input-10-3a49e7393409>:5) ]]\n\nCaused by op u'Placeholder', defined at:\n  File \"/usr/local/Cellar/python@2/2.7.15_3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/local/Cellar/python@2/2.7.15_3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tornado/ioloop.py\", line 1017, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 542, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 456, in _handle_events\n    self._handle_recv()\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 486, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 438, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-3a49e7393409>\", line 5, in <module>\n    x = tf.placeholder(tf.float32, [batch_size, 32,32,3])\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 2077, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5791, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/Users/liyunfan/targetDirectory/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [100,32,32,3]\n\t [[node Placeholder (defined at <ipython-input-10-3a49e7393409>:5) ]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_num):\n",
    "    for batch in range(n_batch):\n",
    "        #batch_ys_onehot = np.eye(10, dtype=float)[batch_labels]\n",
    "        batch_images, batch_labels = get_batch(batch_size,  XtrAll,YtrAll)\n",
    "        batch_images=np.array(batch_images,dtype=np.float)\n",
    "        batch_labels=np.array(label_batch,dtype=np.float)\n",
    "        _, cross_entropy = sess.run([train_op, loss], feed_dict={x_image: batch_images, y_: batch_labels, keep_prob: 1})\n",
    "        if batch % 100 == 0:\n",
    "            print(\"epoch:\" ,str(epoch) ,\" batch:\",batch,'cross_entropy',cross_entropy)\n",
    "#         train_writer.add_summary(summary, epoch*50000+batch)\n",
    "\n",
    "    #每个epoch后，计算训练出的模型在测试集上的准确率\n",
    "    accuracy = sess.run([top_k_op], feed_dict={x_image: XteAll, y_: YteAll, keep_prob: 1})\n",
    "    test_accuracy = float(np.sum(accuracy) / 10000)\n",
    "    test_a.append(test_accuracy)\n",
    "    print(\"epoch:%d  test accuracy %f\" % (epoch, test_accuracy))\n",
    "train_writer.close()\n",
    "saver.save(sess, saveFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
